---
Title: "Assignment 2"
Name: Leon Wu(10582390)
---

```{r}
# Install necessary packages for data analysis and visualization
#install.packages(c("tidyverse","ggpubr","moments","scatterplot3d","factoextra","ranger"))
install.packages(c("tidyverse","caret","ranger","ggpubr"))
```

```{r}
# Load required libraries
library(tidyverse)
library(ggpubr)
# library(moments)
# library(scatterplot3d) 
# library(factoextra)
library(ranger)  #For random forest
library(caret)  #Classification and Regression Training package

library(rpart)
library(rpart.plot)

# Set the working directory
# Note that you may need to change the path to your work directory
setwd("/Users/leonfuns/Projects/ECU/data-analysis/a2-data/a2.try")
options(scipen = 999) # show all numbers
```

## ------------------------------------------------------------------
#        Part 1 – General data preparation and cleaning (a)
## ------------------------------------------------------------------
```{r}
# You may need to change/include the path of your working directory
mydata = read.csv("HealthCareData_2024.csv", stringsAsFactors = TRUE)
dim(mydata)
# ```

## ------------------------------------------------------------------
#        Part 1 – General data preparation and cleaning (b)
## ------------------------------------------------------------------
# ```{r}
## i. Clean the dataset based on the feedback received for Assignment 1.
summary(mydata)

mydata$AlertCategory = fct_collapse(mydata$AlertCategory, 
  Informational = c("Informational", "Info"))
mydata$NetworkEventType = fct_collapse(mydata$NetworkEventType, 
  PolicyViolation = c("Policy_Violation", "PolicyViolation"))

mydata$NetworkAccessFrequency[mydata$NetworkAccessFrequency == -1] = NA

mydata$ResponseTime[mydata$ResponseTime > 150 ] = NA

summary(mydata)
# ```

# ```{r}
## ii. merge the ‘Regular’ and ‘Unknown’ categories together.
mydata$NetworkInteractionType = fct_collapse(mydata$NetworkInteractionType, 
                                  Others = c("Regular", "Unknown"))

summary(mydata)
# ```

# ```{r}
## iii. Standardising dataset.
# Delete the System Access Rate, as this column has too many NA data, 
# and it is a weak discriminator
dat.cleaned = na.omit(mydata[,-9]);dat.cleaned
summary(dat.cleaned)
# ```

## ------------------------------------------------------------------
#        Part 1 – General data preparation and cleaning (c)
## ------------------------------------------------------------------
# ```{r}
# Separate samples of normal and malicious events
dat.class0 = dat.cleaned %>% filter(Classification == "Normal") # normal
dat.class1 = dat.cleaned %>% filter(Classification == "Malicious") # malicious
# Randomly select 9600 non-malicious and 400 malicious samples using your student
# ID, then combine them to form a working data set
set.seed(10582390)
rows.train0 = sample(1:nrow(dat.class0), size = 9600, replace = FALSE)
rows.train1 = sample(1:nrow(dat.class1), size = 400, replace = FALSE)
# Your 10000 ‘unbalanced’ training samples
train.class0 = dat.class0[rows.train0,] # Non-malicious samples
train.class1 = dat.class1[rows.train1,] # Malicious samples
mydata.ub.train = rbind(train.class0, train.class1)
# Your 19200 ‘balanced’ training samples, i.e. 9600 normal and malicious samples each.
set.seed(10582390)
train.class1_2 = train.class1[sample(1:nrow(train.class1), size = 9600,
  replace = TRUE),]
mydata.b.train = rbind(train.class0, train.class1_2)
# Your testing samples
test.class0 = dat.class0[-rows.train0,]
test.class1 = dat.class1[-rows.train1,]
mydata.test = rbind(test.class0, test.class1)
# ```

## ------------------------------------------------------------------
#   Part 2 – Compare the performances of different ML algorithms (a)
## ------------------------------------------------------------------
# ```{r}
set.seed(10582390)
models.list1 = c("Logistic Ridge Regression",
  "Logistic LASSO Regression",
  "Logistic Elastic-Net Regression")
models.list2 = c("Classification Tree",
  "Bagging Tree",
  "Random Forest")
myModels = c(sample(models.list1, size = 1),
  sample(models.list2, size = 1))
myModels %>% data.frame
# ```

#```{r}
#write.csv(mydata.ub.train, "mydata.ub.train.csv")
#write.csv(mydata.b.train, "mydata.b.train.csv")
#write.csv(mydata.test, "mydata.test.csv")
# write.csv(mydata, "mydata.csv")
```

```{r}
#A sequence of lambdas
lambdas = 10^seq(-3, 3, length = 100)
alphas = seq(0.1,0.9,by=0.1)

set.seed(10582390)
mod.ridge.b = train(
  Classification~.,
  data = mydata.b.train,
  method = "glmnet",
  preProcess = NULL,
  trControl = trainControl("repeatedcv", number = 10, repeats = 2),
  tuneGrid = expand.grid(alpha = alphas, lambda = lambdas)
)
```

```{r}
set.seed(10582390)
mod.ridge.ub = train(
  Classification~.,
  data = mydata.ub.train,
  method = "glmnet",
  preProcess = NULL,
  trControl = trainControl("repeatedcv", number = 10, repeats = 2),
  tuneGrid = expand.grid(alpha = alphas, lambda = lambdas)
)
mod.ridge.b$bestTune;mod.ridge.ub$bestTune
```

```{r}
gg.en.b = ggplot(mod.ridge.b$results, aes(x = lambda, y = Accuracy, color = factor(alpha))) +
    geom_point() + geom_line() + scale_x_log10() +
    labs(title = "Elastic-Net Tuning: Balanced Data Set",
         x = "Lambda (log scale)",
         y = "Accuracy",
         color = "Alpha") +
    theme_minimal()

gg.en.ub = ggplot(mod.ridge.ub$results, aes(x = lambda, y = Accuracy, color = factor(alpha))) +
    geom_point() + geom_line() + scale_x_log10() +
    labs(title = "Elastic-Net Tuning: Unbalanced Data Set",
         x = "Lambda (log scale)",
         y = "Accuracy",
         color = "Alpha") +
    theme_minimal()
gg.en.b
gg.en.ub
```

```{r}
# Model coefficients
coef(mod.ridge.b$finalModel, mod.ridge.b$bestTune$lambda)
coef(mod.ridge.ub$finalModel, mod.ridge.ub$bestTune$lambda)
```

```{r}
pred.class.b = predict(mod.ridge.b,new=mydata.test)
pred.class.ub = predict(mod.ridge.ub,new=mydata.test)

cf.b = table(relevel(pred.class.b, ref="Malicious"), 
             relevel(mydata.test$Classification, ref="Malicious"))
cf.ub = table(relevel(pred.class.ub, ref="Malicious"),
             relevel(mydata.test$Classification, ref="Malicious"))

prop.b = round(prop.table(cf.b, 2), digits = 3);prop.b
prop.ub = round(prop.table(cf.ub, 2), digits = 3);prop.ub

```

```{r}
cm.cf.b = confusionMatrix(cf.b,mode="everything");cm.cf.b
cm.cf.ub = confusionMatrix(cf.ub,mode="everything");cm.cf.ub
```

## ------------------------------------------------------------------
#                   Part 2 – RANDOM FOREST
## ------------------------------------------------------------------
```{r}
mod.rf.b = ranger(
  Classification~.,
  data = mydata.b.train,
  num.trees = 500,
  mtry = 3,
  respect.unordered.factors = TRUE,
  seed = 10582390,
  importance = "impurity"
)
mod.rf.ub = ranger(
  Classification~.,
  data = mydata.ub.train,
  num.trees = 500,
  mtry = 3,
  respect.unordered.factors = TRUE,
  seed = 10582390,
  importance = "impurity"
)
```

```{r}
mod.rf.b;mod.rf.ub
```

```{r}
pred.mod.rf.b = predict(mod.rf.b, data = mydata.test);pred.mod.rf.b
pred.mod.rf.ub = predict(mod.rf.ub, data = mydata.test);pred.mod.rf.ub
```

```{r}
rf.b.t = table(relevel(pred.mod.rf.b$predictions, ref="Malicious"), 
             relevel(mydata.test$Classification, ref="Malicious"))
rf.ub.t = table(relevel(pred.mod.rf.ub$predictions, ref="Malicious"),
             relevel(mydata.test$Classification, ref="Malicious"))

prop.rf.b = round(prop.table(rf.b.t, 2), digits = 3);prop.rf.b
prop.rf.ub = round(prop.table(rf.ub.t, 2), digits = 3);prop.rf.ub
```


```{r}
cm.b = confusionMatrix(rf.b.t, mode="everything");cm.b
cm.ub = confusionMatrix(rf.ub.t, mode="everything");cm.ub
```

```{r}
# num.trees = c(300, 400, 500), mtry = c(3:5),
# num.trees = c(200, 300, 400, 500), mtry = c(3:11)
grid.rf = expand.grid(num.trees = c(200, 300, 400, 500), mtry = c(3:11),
                      min.node.size = seq(2, 10, 2), replace = c(TRUE, FALSE),
                      sample.fraction = c(0.5, 0.6, 0.7, 0.8, 1),
                      OOB.misclass = NA, accuracy = NA, b_accuracy = NA,
                      specificity = NA, precision = NA, reccall = NA,
                      f1_score = NA, tp = NA, fp = NA, fn = NA, tn = NA)
dim(grid.rf);grid.rf
```

```{r}
rf.train = function(data.train) {
  for (I in 1:nrow(grid.rf)) {
    rf = ranger(Classification ~ .,
                data = data.train,
                num.trees = grid.rf$num.trees[I],
                mtry = grid.rf$mtry[I],
                min.node.size = grid.rf$min.node.size[I],
                replace = grid.rf$replace[I],
                sample.fraction = grid.rf$sample.fraction[I],
                seed = 10582390,
                respect.unordered.factors = "order")

    grid.rf$OOB.misclass[I] = rf$prediction.error %>% round(5) * 100

    pred.test = predict(rf, data = mydata.test)$predictions

    test.cf = confusionMatrix(relevel(pred.test, ref="Malicious"),
              relevel(mydata.test$Classification, ref="Malicious"))
    
    grid.rf$accuracy[I] = test.cf$overall["Accuracy"]
    grid.rf$b_accuracy[I] = test.cf$byClass["Balanced Accuracy"]
    grid.rf$specificity[I] = test.cf$byClass["Specificity"]
    grid.rf$precision[I] = test.cf$byClass["Precision"]
    grid.rf$reccall[I] = test.cf$byClass["Recall"]
    grid.rf$f1_score[I] = test.cf$byClass["F1"]
    grid.rf$tp[I] = test.cf$table[1,1]
    grid.rf$fp[I] = test.cf$table[2,1]
    grid.rf$fn[I] = test.cf$table[1,2]
    grid.rf$tn[I] = test.cf$table[2,2]
  }
  #Return top 10 sorted results by the OOB misclassification error
  return(grid.rf[order(grid.rf$OOB.misclass, decreasing = FALSE)[1:10],])
}
```

```{r}
train.rf.b = rf.train(mydata.b.train);train.rf.b
```

```{r}
train.rf.ub.t = rf.train(mydata.ub.train);train.rf.ub.t
```

```{r}
train.rf.b;train.rf.ub.t
```

```{r}
confusion.matrix = function(train.rf){
  cm.vec = c(train.rf[1,13],train.rf[1,15],train.rf[1,14],train.rf[1,16])
  cm.rf = matrix(cm.vec, nrow = 2, byrow = TRUE)
  rownames(cm.rf) = c("Malicious","Normal")
  colnames(cm.rf) = c("Malicious","Normal")
  cm.rf
}
```

```{r}
cm.rf.b = confusion.matrix(train.rf.b)
cm.rf.ub = confusion.matrix(train.rf.ub.t)

cm.rf.b.r = round(prop.table(cm.rf.b, 2), digits = 3)
cm.rf.ub.r = round(prop.table(cm.rf.ub, 2), digits = 3)

cm.rf.b;cm.rf.ub;cm.rf.b.r;cm.rf.ub.r
```

```{r}
train.rf.b$rownum = rownames(train.rf.b)

gg.rf.b = ggplot(train.rf.b, 
       aes(x = rownum, y = OOB.misclass, color = as.factor(num.trees), 
           shape = as.factor(min.node.size), group = 1)) + geom_point(size = 3) +
       geom_line() +
       geom_text(aes(label = paste("mtry:", as.character(mtry))), vjust = 1.5, size = 3) +
       labs(title = "Random Forest: Hyperparameter Tuning/Search Strategy for balance dataset",
       x = "Top 10 Hyperparameter Tuning/Search Grid", 
       y = "OOB misclassification error") +
  theme_bw()
gg.rf.b
```


```{r}
train.rf.ub.t$rownum = rownames(train.rf.ub.t)

gg.rf.ub = ggplot(train.rf.ub.t, 
       aes(x = rownum, y = OOB.misclass, color = as.factor(num.trees), 
           shape = as.factor(min.node.size), group = 1)) + geom_point(size = 3) +
       geom_line() +
       geom_text(aes(label = paste("mtry:", as.character(mtry))), vjust = 1.5, size = 3) +
       labs(title = "Random Forest: Hyperparameter Tuning/Search Strategy for unbalance dataset",
       x = "Top 10 Hyperparameter Tuning/Search Grid", 
       y = "OOB misclassification error") +
  theme_bw()
gg.rf.ub
```

```{r}
output.img = function(x,y) {
  ggexport(x, filename=y ,width = 512 ,height = 384)
}

output.img(gg.en.b,"Elastic-Net-Tuning-balance.png")
output.img(gg.en.ub,"Elastic-Net-Tuning-unbalance.png")
output.img(gg.rf.b,"Random-Forest-Tuning-balance.png")
output.img(gg.rf.ub,"Random-Forest-Tuning-unbalance.png")
```

```{r}
options(scipen = 999)
# ?geom_line()
cm.cf.b$table
cm.cf.b$table[1,1] # TP
cm.cf.b$table[2,1] # FP
cm.cf.b$table[1,2] # FN
cm.cf.b$table[2,2] # TN
?confusionMatrix
summary(mydata.test)

save.image(file = "workspace.RData")
load("workspace.RData")
```






